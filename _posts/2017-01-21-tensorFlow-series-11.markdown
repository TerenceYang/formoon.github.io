---
layout:         page
title:          从锅炉工到AI专家(11)(END)
subtitle:       TensorFlow实务
card-image:     http://p1avd6u2z.bkt.clouddn.com/201801/ml/tensorflowlogo.jpg
date:           2017-01-20
tags:           ml toSeven
post-card-type: image
---
#### TensorFlow剩下的那些话题
我们不断的重复，机器学习本质就是以数学为代表的最新成就在计算机算法上的展现，因此TensorFlow也好，Matlab/Octave也好，所有的机器学习软件包，都具备非常强悍的数学计算能力。  
因此除了前面讲到的这些，用TensorFlow解决数学问题也非常有优势。  
官方提供的教程中，后续还有两个这方面的应用，因为与本课程的定位相关性比较弱，建议有兴趣的读者去官方的教程学习。喜欢看中文的读者也可以参考TensorFlow中文社区的翻译版本。  
[TensorFlow中文社区：曼德布洛特(Mandelbrot)集合](http://www.tensorfly.cn/tfdoc/tutorials/mandelbrot.html)  
[TensorFlow中文社区：偏微分方程](http://www.tensorfly.cn/tfdoc/tutorials/pdes.html)  

此外就是TensorFlow的应用方面的特征，也就是官方教程中的进阶部分。这里面我们只讲解了非常必须的TensorBoard。  
其它的部分，比如队列、线程、GPU计算，也非常重要。但是在你的学习阶段可以先不关注，等到项目正式成型，进入产品研发的阶段，相信随着原型设计阶段对TensorFlow的使用，你已经比较熟悉了，抽上很短的时间看看就能让你用起来。而且这部分官方的教程已经很好了，我也做不到锦上添花:)  

#### 他山之石
TensorFlow的确是非常优秀的机器学习框架，上手非常容易，资源丰富，效率也非常高。  
但是其它的竞争软件包也一直在努力，并且往往有自己的独到之处。  

_OpenCV_ 是我原来经常用的一款软件包，开源免费，全部由c/c++写成，效率超高。  
OpenCV最初定位是“计算机视觉”，在这方面走的很远，内置了很多功能可以直接对照片、视频、摄像头等进行处理。这种内置可比python利用第三方软件包完成的功能更易用，完全是一体的感觉。  
所以有一些图像识别类的算法在OpenCV上的实现更早、更新更快。比如AR中的物品追踪、人像识别，OpenCV的实现和TensorFlow的实现基本同样是社区的贡献，但OpenCV的版本因为更新更快，往往有更强的功能。  
例如对某个物体的识别，机器学习的思路我们应当很熟悉了，需要很多已标注的样本，经过调优、训练完成模型，就可以用来进行识别。但是在很多情况下，你根本没有这么多的样本。OpenCV同样可以使用“人工智能”的某些非“机器学习”范畴的方式，不用很多样本也同样可以做到物体的识别追踪。这些其实已经不是TensorFlow不足，而是“机器学习”在这些方面的不足。  
与此类似，很多图像预处理的工作也经常习惯先用OpenCV做一遍。并不是说python利用扩展库无法完成，而是因为更方便。  
在本博中有几篇博文，展示了使用OpenCV在图像处理方面的优势，有兴趣的也可以去看看，体验一下我说的“方便”。  
  
_Octave_ 这个软件包和它模仿的商业版本Matlab我们已经一再提及了，非学术界的纯粹程序员大多都有点小看它，实际上很多时候，配合上强悍的数学公式，这个软件包能做到的事情往往出人意料。  
我收藏有一个吴恩达介绍的公式，仅用一行代码解决“鸡尾酒会问题”。  
这个问题是指，在类似鸡尾酒会这种人多、嘈杂的环境下，有两个麦克风。其中一个麦克风附近，有人在对话；另外一个麦克风附近，是一个小提琴手在演奏。  
如果是真实的人类的话，大脑会本能的、自动的过滤掉背景嘈杂的声音，从中选取自己想听到的资讯。  
而“人工智能”处理这个问题就难了很多。不过通过数学家的帮助，如前所述场景，利用两个麦克风所得到的声源中音量的区别，Octave可以用一行公式把两个声音区分出来:  
```Matlab
[W,s,v] = svd((repmat(sum(yy.*yy,1),size(yy,1),1).*yy)*yy');
```
当然这里仅是示例，上面的公式跑起来，还需要附加处理的配合，诸如音源输入、预处理、分离之后的音频输出等。  

还有一些软件包，是在机器学习尚未火爆的年代就启动，做了很多年的研究，专注于解决某一方面的问题并且有很高成就的。比如分离同一个信号源中混杂的多个不同频点的信号（Independent component analysis），已经有成熟的专用软件包FastICA解决，同样可以应用在“鸡尾酒会问题”的解决中。  

所以这一节想要强调的，“机器学习”之外，实际上还有很多值得关注的技术、算法。千万不要“手里有把锤子，看什么都是钉子。”  

#### 其它那些机器学习框架
除了TensorFlow，当前还有很多机器学习框架广为应用，或者正在研发中。其中有些重量级的产品你也应当知道。  
__Caffe__: 卷积神经网络框架，专注于卷积神经网络和图像处理，用C++语言写成的。一个新的由Facebook 支持的Caffe迭代版本称为Caffe2，现在正在开发过程中，即将进行1.0发布。其目标是为了简化分布式训练和移动部署，提供对于诸如FPGA等新类型硬件的支持，并且利用先进的如16位浮点数训练的特性。   
__Chainer__: 一个强大、灵活、直观的机器学习Python软件库，能够在一台机器上利用多个GPU。是由深度学习创业公司 Preferred Networks 开发；Chainer 的设计基于 define by run 原 则，也就是说，该网络在运行中动态定义，而不是在启动时定义。  
__DMTK__: 微软的DMTK(分布式机器学习工具集)框架解决了在系统集群中分布多种机器学习任务的问题。  
__CNTK__: 微软研究人员开发的用于深度神经网络和多GPU加速技术的完整开源工具包。微软称CNTK在语音和图像识别方面，比谷歌的 TensorFlow 等其它深度学习开源工具包更有优势。  
__Deeplearning4j__: 专注于神经网络的 Java 库，可扩展并集成 Spark，Hadoop 和其他基于 Java 的分布式集成软件。  
__Nervana Neo__: 是一个高效的 Python 机器学习库，它能够在单个机器上使用多个GPU。  
__Theano__: 是一个用 Python 编写的极其灵活的 Python 机器学习库，用它定义复杂的模型相当容易，因此它在研究中极其流行。  
__Torch__: 是一个专注于 GPU 实现的机器学习库，得到了几个大公司的研究团队的支持。  
__Apache Spark MLlib__: 看名字就知道，Apache基金会的开源系统，来源于Spark中面向数学和统计用户的平台，同Spark的兼容性毋庸置疑。  
__H2O__: 现在已经发展到第三版，可以提供通过普通开发环境(Python, Java, Scala, R)、大数据系统(Hadoop, Spark）以及数据源(HDFS, S3, SQL, NoSQL)访问机器学习算法的途径。H2O是用于数据收集、模型构建以及服务预测的端对端解决方案。  
__Singa__: 是一个Apache的孵化器项目，也是一个开源框架，作用是使在大规模数据集上训练深度学习模型变得更简单。Singa提供了一个简单的编程模型，用于在机器群集上训练深度学习网络，它支持很多普通类型的训练工作：卷积神经网络，受限玻尔兹曼机 以及循环神经网络。 模型可以同步训练（一个接一个）或者也异步（一起）训练，也可以允许在在CPU和GPU群集上，很快也会支持FPGA。Singa也通过Apache Zookeeper简化了群集的设置。   
__Turi Create__：苹果公司发布，可以帮苹果几大操作系统上所运行软件的开发者，构建用于推荐、对象检测、图像分类、图像相似性以及活动分类的机器学习模型。  
__CoreML__: 同样是苹果发布，针对IOS系统，可以利用到苹果内置的机器学习CPU功能，在IOS上做机器学习首选框架。  

##### 云端产品：  
__亚马逊的机器学习服务__: 不同于前面那些，亚马逊的机器学习主要以服务的形式提供，就类似于它的其它云服务产品。该服务可以连接到存储在亚马逊 S3、Redshift或RDS上的数据，并且在这些数据上运行二进制分类、多级分类或者回归以构建一个模型。但是，值得注意的是生成的模型不能导入或导出，而训练模型的数据集不能超过100GB。亚马逊的深度学习机器图景包含了许多主要的深度学习框架，包括 Caffe2、CNTK、MXNet和TensorFlow。   
__微软的Azure ML Studio__: 考虑到执行机器学习所需的大量数据和计算能力，对于机器学习应用云是一种理想环境。微软已经为Azure配备了自己的即付即用的机器学习服务-Azure ML Studio，提供了按月、按小时和免费的版本。(该公司的HowOldRobot项目就是利用这个系统创立的。)你甚至不需要一个账户来就可以试用这项服务;你可以匿名登录，免费使用Azure ML Studio最多8小时。  
国内的百度、腾讯、阿里，也都有了自己一些机器学习的云端产品值得大家关注。  

机器学习的产品和框架如同雨后春笋般层出不穷，此处肯定不能尽列，我的建议是关注大公司产品，一般就能跟上风向。另外就是关注特定设备的特定应用，比如你在iPhone上做研发，那使用CoreML可能就是必要的选择，否则无法用到CPU内置的机器学习功能，出来的产品也会更费电、更慢。  
  
#### 新概念  
快速发展的机器学习领域经常有一些新的技术，比如：  
__增强学习__：举一个例子更能说明。比如在一盘围棋中，盘中的某一步，实际上并不一定代表对于最终结果更优或者更差。  
传统方法是使用大规模的棋谱训练模型，来对下一步的选择做出预测。  
而增强学习是随机生成下一步的选择，然后以综合全局胜者累计增加1分，败者累计减去1分，从而在全局的角度上进行学习。  
因为电脑运算速度快，可能很快的进行非常大量的模拟棋局，从而让模型具备较好的棋局预测能力。  
最新的AlphaGo Zero就是典型代表。但这种模式对于训练的数据量需求是非常非常庞大的，在无法自动生成数据集的场景中尚需继续探索。
  
__迁移学习__：这种方法实际一直在应用，只是目前有了一些更规范和更具效果的方法。顾名思义就是就是把已经训练好的模型参数迁移到新的模型来帮助新模型训练。  
比如在自动驾驶的训练中，如果我们一直用白天的视频进行训练，如果到了晚上，同一个地点，可能自动驾驶系统的表现就会非常差。  
而迁移学习就是利用同样的数据集，经过各种变换和预处理以及算法的改进，虽然并不能完全替代用夜晚数据的训练，但可以明显加强不同环境下模型的表现，这就是迁移学习。  
通常我们在图像识别应用中，为了增加数据集，也会把图片适当的调整角度、调整位置、调整颜色、增加噪点。这些方法今天来看实际也属于迁移学习的范畴。  

__多任务学习__：跟上一中模式是比较相似的。在某个模型的训练中，同时考虑到应用的多个场景，并行的进行训练，从而让完全不同的数据集发挥更大的作用，增强模型的适应性。  

#### 结束语
非常感谢你读到最后，这里我们也进展到了尾声。  
2017是人工智能的元年，这一点是可以确认的。  
但今后并不一定就是人工智能的时代。人工智能尚处于发展中，很多问题需要研究，很多难点尚未突破。所以，这是我们更需要努力的地方。  
个人在实践中的感觉，不要随时都抱着一种试图“颠覆一切”的心态。  
“人工智能”和“机器学习”最大的价值，在于除了传统的编程模式，又多了一套完整的理论、框架体系，让我们有了更多的选择来解决现实中的实际问题。实践起来可能更快、更好。  
对于发展时间还很短的“机器学习”框架来说，我们都是初学者。希望我们能互相帮助、互相促进。  
水平有限，错误、疏漏在所难免，希望得到大家的批评指正，多谢。  

(END)
#### 引文及参考  
[增强学习、增量学习、迁移学习](http://blog.csdn.net/zyazky/article/details/51942135)  

